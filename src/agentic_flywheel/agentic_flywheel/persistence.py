"""
Agentic Flywheel Persistence Module

This module enables users to create a persistent and cumulative knowledge base
from their interactions with the Agentic Flywheel MCP. By integrating Redis
for storage, we manifest continuous conversations, instantaneous insight
retrieval, and a compounding institutional memory.

Based on redis-storage.spec.md (v1.0) - RISE Framework
"""

import hashlib
import json
import logging
import time
from typing import Dict, Any, Optional, List
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)


class PersistenceManager:
    """
    Manifests persistent memory and intelligent caching for the Agentic Flywheel.

    This class transforms ephemeral conversations into enduring knowledge artifacts,
    creating a self-reinforcing loop of knowledge accumulation and accelerated retrieval.
    """

    # TTL configurations (in seconds) - enables knowledge to stay relevant
    TTL_TECHNICAL = 7 * 24 * 60 * 60  # 7 days for technical flows
    TTL_CREATIVE = 30 * 24 * 60 * 60  # 30 days for creative/strategic flows
    TTL_VOLATILE = 24 * 60 * 60  # 1 day for frequently updated content

    # Flow type mappings - determines caching strategy
    FLOW_TTL_MAP = {
        'technical-analysis': TTL_TECHNICAL,
        'technical': TTL_TECHNICAL,
        'code-review': TTL_TECHNICAL,
        'creative-orientation': TTL_CREATIVE,
        'strategic-planning': TTL_CREATIVE,
        'brainstorming': TTL_CREATIVE,
        'document-qa': TTL_VOLATILE,
        'real-time-data': TTL_VOLATILE,
    }

    def __init__(self, redis_client=None, enable_mcp_tools: bool = True):
        """
        Initialize the persistence manager.

        Args:
            redis_client: Optional direct Redis client (for testing/custom setups)
            enable_mcp_tools: Whether to use MCP tools (coaia_tash/coaia_fetch)
        """
        self.redis_client = redis_client
        self.enable_mcp_tools = enable_mcp_tools
        self._cache_stats = {
            'hits': 0,
            'misses': 0,
            'writes': 0,
            'errors': 0
        }

    def generate_cache_key(self, session_id: str, question: str, flow_id: str) -> str:
        """
        Generates a consistent, deterministic cache key for a query.

        This key uniquely identifies a question within a session and flow context,
        enabling reliable retrieval of cached responses while avoiding collisions.

        Args:
            session_id: Unique identifier for the conversation session
            question: The user's question text
            flow_id: Identifier for the Flowise flow used

        Returns:
            A namespaced cache key string

        Example:
            >>> pm = PersistenceManager()
            >>> key = pm.generate_cache_key(
            ...     "mcp-session-123",
            ...     "How do I deploy to production?",
            ...     "technical-analysis"
            ... )
            >>> key
            'afw:session:mcp-session-123:flow:technical-analysis:q_hash:a1b2c3d4...'
        """
        # Normalize the question to ensure consistency
        normalized_question = question.lower().strip()
        question_hash = hashlib.md5(normalized_question.encode()).hexdigest()

        # Create a namespaced key following Redis best practices
        cache_key = f"afw:session:{session_id}:flow:{flow_id}:q_hash:{question_hash}"

        logger.debug(f"Generated cache key: {cache_key[:50]}... for question: {question[:50]}...")
        return cache_key

    async def check_cache(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """
        Retrieves a cached response if it exists.

        This enables instantaneous insight retrieval, transforming the system
        into a "second brain" that accelerates learning and decision-making.

        Args:
            cache_key: The cache key generated by generate_cache_key()

        Returns:
            The cached response dictionary if found, None otherwise

        Example:
            >>> cached = await pm.check_cache(cache_key)
            >>> if cached:
            ...     print(f"Cache hit! Retrieved in {cached['_cache_metadata']['retrieval_time']}ms")
        """
        try:
            if self.enable_mcp_tools:
                # Use MCP coaia_fetch tool
                from mcp import types
                # Note: In actual implementation, this would call the MCP tool
                # For now, we'll use the redis_client if available
                if self.redis_client:
                    cached_value = self.redis_client.get(cache_key)
                else:
                    # Placeholder for MCP tool call
                    logger.warning("MCP tools not fully integrated yet, using fallback")
                    cached_value = None
            else:
                cached_value = self.redis_client.get(cache_key) if self.redis_client else None

            if cached_value:
                # Parse the stored JSON
                response_data = json.loads(cached_value) if isinstance(cached_value, (str, bytes)) else cached_value

                # Augment with cache hit metadata for observability
                response_data['_cache_metadata'] = {
                    'cache_hit': True,
                    'retrieval_time': datetime.utcnow().isoformat(),
                    'cache_key': cache_key
                }

                self._cache_stats['hits'] += 1
                logger.info(f"âœ… Cache HIT for key: {cache_key[:50]}... (Hit rate: {self.get_hit_rate():.1%})")
                return response_data
            else:
                self._cache_stats['misses'] += 1
                logger.info(f"âŒ Cache MISS for key: {cache_key[:50]}... (Hit rate: {self.get_hit_rate():.1%})")
                return None

        except Exception as e:
            self._cache_stats['errors'] += 1
            logger.error(f"Error checking cache for key {cache_key}: {str(e)}")
            return None

    async def write_to_cache(
        self,
        cache_key: str,
        response_data: Dict[str, Any],
        ttl: Optional[int] = None,
        flow_key: Optional[str] = None
    ) -> bool:
        """
        Stores a response in the cache, creating a knowledge artifact.

        This transforms ephemeral LLM responses into enduring institutional memory,
        building a compounding knowledge base that grows more valuable over time.

        Args:
            cache_key: The cache key generated by generate_cache_key()
            response_data: The complete response from Flowise (including metadata)
            ttl: Optional custom TTL in seconds
            flow_key: Flow identifier to determine TTL if not explicitly provided

        Returns:
            True if successfully cached, False otherwise

        Example:
            >>> success = await pm.write_to_cache(
            ...     cache_key,
            ...     flowise_response,
            ...     flow_key="technical-analysis"
            ... )
            >>> if success:
            ...     print("Knowledge artifact created!")
        """
        try:
            # Determine TTL based on flow type if not explicitly provided
            if ttl is None:
                ttl = self.get_ttl_for_flow(flow_key or 'default')

            # Add caching metadata to the response
            cache_metadata = {
                '_cache_created': datetime.utcnow().isoformat(),
                '_cache_ttl': ttl,
                '_cache_expires': (datetime.utcnow() + timedelta(seconds=ttl)).isoformat()
            }

            # Merge cache metadata without modifying original
            cached_response = {**response_data, **cache_metadata}

            # Serialize to JSON
            cached_value = json.dumps(cached_response, ensure_ascii=False)

            if self.enable_mcp_tools:
                # Use MCP coaia_tash tool
                if self.redis_client:
                    self.redis_client.setex(cache_key, ttl, cached_value)
                else:
                    # Placeholder for MCP tool call
                    logger.warning("MCP tools not fully integrated yet, using fallback")
                    return False
            else:
                if self.redis_client:
                    self.redis_client.setex(cache_key, ttl, cached_value)
                else:
                    return False

            self._cache_stats['writes'] += 1
            logger.info(f"ðŸ’¾ Cache WRITE successful for key: {cache_key[:50]}... (TTL: {ttl}s / {ttl/86400:.1f} days)")
            return True

        except Exception as e:
            self._cache_stats['errors'] += 1
            logger.error(f"Error writing to cache for key {cache_key}: {str(e)}")
            return False

    def get_ttl_for_flow(self, flow_key: str) -> int:
        """
        Determines the appropriate TTL based on the flow type.

        Different types of knowledge have different shelf lives - technical
        implementations are relatively stable, while real-time data needs
        frequent refreshing.

        Args:
            flow_key: The flow identifier (e.g., 'technical-analysis')

        Returns:
            TTL in seconds

        Example:
            >>> ttl = pm.get_ttl_for_flow('technical-analysis')
            >>> print(f"Technical knowledge cached for {ttl/86400} days")
        """
        return self.FLOW_TTL_MAP.get(flow_key, self.TTL_CREATIVE)

    def format_cached_response(self, cached_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Formats a cached response for return to the user.

        Ensures cached responses include appropriate metadata to distinguish
        them from fresh LLM responses, enabling observability and analytics.

        Args:
            cached_data: The raw cached response data

        Returns:
            Formatted response with cache metadata
        """
        # Ensure cache_hit metadata is present
        if '_cache_metadata' not in cached_data:
            cached_data['_cache_metadata'] = {
                'cache_hit': True,
                'retrieval_time': datetime.utcnow().isoformat()
            }

        return cached_data

    async def get_session_history(
        self,
        session_id: str,
        limit: int = 5
    ) -> List[Dict[str, Any]]:
        """
        Retrieves recent conversation history for a session.

        This enables continuous, uninterrupted conversations where the system
        has perfect recall of prior context, as if no time has passed.

        Args:
            session_id: The session identifier
            limit: Maximum number of previous interactions to retrieve

        Returns:
            List of previous response dictionaries, most recent first

        Example:
            >>> history = await pm.get_session_history("mcp-session-123", limit=5)
            >>> print(f"Retrieved {len(history)} previous interactions")
            >>> # Use history to enrich new queries with context
        """
        try:
            # Scan for all keys matching this session
            pattern = f"afw:session:{session_id}:*"

            if self.redis_client:
                # Get all keys for this session
                keys = list(self.redis_client.scan_iter(match=pattern, count=100))

                # Get all values and parse timestamps
                history_items = []
                for key in keys:
                    value = self.redis_client.get(key)
                    if value:
                        data = json.loads(value)
                        # Extract creation time for sorting
                        created = data.get('_cache_created', data.get('_mcp_metadata', {}).get('timestamp'))
                        if created:
                            history_items.append({
                                'data': data,
                                'created': created
                            })

                # Sort by creation time, most recent first
                history_items.sort(key=lambda x: x['created'], reverse=True)

                # Return only the data portion, limited to requested count
                return [item['data'] for item in history_items[:limit]]
            else:
                logger.warning("No Redis client available for session history retrieval")
                return []

        except Exception as e:
            logger.error(f"Error retrieving session history for {session_id}: {str(e)}")
            return []

    async def invalidate_flow_cache(self, flow_id: str) -> int:
        """
        Clears all cached entries for a specific flow.

        This is called when a flow's configuration is updated, ensuring
        that stale knowledge doesn't persist after changes.

        Args:
            flow_id: The flow identifier to invalidate

        Returns:
            Number of cache entries cleared

        Example:
            >>> cleared = await pm.invalidate_flow_cache("technical-analysis")
            >>> print(f"Cleared {cleared} cached entries after flow update")
        """
        try:
            pattern = f"afw:session:*:flow:{flow_id}:*"

            if self.redis_client:
                keys = list(self.redis_client.scan_iter(match=pattern, count=1000))
                if keys:
                    deleted = self.redis_client.delete(*keys)
                    logger.info(f"ðŸ—‘ï¸  Invalidated {deleted} cache entries for flow: {flow_id}")
                    return deleted
                return 0
            else:
                logger.warning("No Redis client available for cache invalidation")
                return 0

        except Exception as e:
            logger.error(f"Error invalidating cache for flow {flow_id}: {str(e)}")
            return 0

    async def invalidate_session_cache(self, session_id: str) -> int:
        """
        Clears all cached entries for a specific session.

        Args:
            session_id: The session identifier to invalidate

        Returns:
            Number of cache entries cleared
        """
        try:
            pattern = f"afw:session:{session_id}:*"

            if self.redis_client:
                keys = list(self.redis_client.scan_iter(match=pattern, count=1000))
                if keys:
                    deleted = self.redis_client.delete(*keys)
                    logger.info(f"ðŸ—‘ï¸  Invalidated {deleted} cache entries for session: {session_id}")
                    return deleted
                return 0
            else:
                logger.warning("No Redis client available for cache invalidation")
                return 0

        except Exception as e:
            logger.error(f"Error invalidating cache for session {session_id}: {str(e)}")
            return 0

    def get_cache_stats(self) -> Dict[str, Any]:
        """
        Returns current cache performance statistics.

        These metrics reveal the value created by the caching system,
        showing how knowledge accumulation accelerates the creative process.

        Returns:
            Dictionary with cache performance metrics

        Example:
            >>> stats = pm.get_cache_stats()
            >>> print(f"Cache hit rate: {stats['hit_rate']:.1%}")
            >>> print(f"Total knowledge artifacts: {stats['total_requests']}")
        """
        total = self._cache_stats['hits'] + self._cache_stats['misses']
        hit_rate = (self._cache_stats['hits'] / total) if total > 0 else 0.0

        return {
            'hits': self._cache_stats['hits'],
            'misses': self._cache_stats['misses'],
            'writes': self._cache_stats['writes'],
            'errors': self._cache_stats['errors'],
            'total_requests': total,
            'hit_rate': hit_rate,
            'status': 'healthy' if self._cache_stats['errors'] < total * 0.05 else 'degraded'
        }

    def get_hit_rate(self) -> float:
        """Returns the current cache hit rate as a decimal (0.0 to 1.0)."""
        total = self._cache_stats['hits'] + self._cache_stats['misses']
        return (self._cache_stats['hits'] / total) if total > 0 else 0.0

    def reset_stats(self) -> None:
        """Resets cache statistics counters."""
        self._cache_stats = {
            'hits': 0,
            'misses': 0,
            'writes': 0,
            'errors': 0
        }
        logger.info("ðŸ“Š Cache statistics reset")


# Singleton instance for use across the application
_persistence_manager = None


def get_persistence_manager(
    redis_client=None,
    enable_mcp_tools: bool = True
) -> PersistenceManager:
    """
    Returns the singleton persistence manager instance.

    This ensures consistent cache statistics and connection management
    across all parts of the application.

    Args:
        redis_client: Optional Redis client instance
        enable_mcp_tools: Whether to enable MCP tool integration

    Returns:
        The global PersistenceManager instance
    """
    global _persistence_manager

    if _persistence_manager is None:
        _persistence_manager = PersistenceManager(
            redis_client=redis_client,
            enable_mcp_tools=enable_mcp_tools
        )
        logger.info("âœ¨ Persistence manager initialized - ready to create knowledge artifacts!")

    return _persistence_manager
